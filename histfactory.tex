

\chapter{HistFactory}
\label{app:histfactory}

Statistical measurements, including estimates of physical parameters, the setting of confidence intervals, and the setting of upper and lower limits, require the construction of a likelihood function that incorporates one's knowledge of physical theories, experimental properties, and underlying uncertainties.
Moreover, it is a practical requirement that such a likelihood be expressible in software, and that the software be simple, widely applicable, and fast.
To make complicated statistical techniques widely available and to enable the construction of models complicated enough to accurately describe the details of physical theories, particle detectors, and the LHC, physicists built a software library consisting of tools specifically designed for the creation and evaluation of likelihood functions.

HistFactory is a software tool designed to build paramaterized probability density functions and likelihood functions and is specifically designed to use histograms as inputs~\cite{Cranmer:1456844}.
HistFactory is written in c++, is a part of the ROOT framework, and was built using a statistical modeling library called RooFit.
It can be interfaced with as a c++ library or using both XML or python as a front end.
HistFactory produces standard but highly flexible and customizable likelihood functions using minimal input from the end user.
It does this by taking a modular approach to the construction of probability density functions, where terms in a distribution describing different measurements can be built individually or combined to form larger distributions that describe a bigger space of measurements.

HistFactory is designed to allow systematic uncertainties, both theoretical or experimental in source, to be built into a likelihood function and treated consistently throughout a probability density function, properly taking into account correlations between effects that arise from a single source of systematic error.

%% The \HF\ is a tool to build parametrized probability density functions (pdfs) in the \RooFit/\RooStats\ framework based based on simple ROOT histograms organized in an XML file.  The pdf has a restricted form, but it is sufficiently flexible to describe many analyses based on template histograms. The tool takes a modular approach to build complex pdfs from more primative conceptual building blocks.  The resulting PDF is stored in a RooWorkspace which can be saved to and read from a ROOT file.  This document describes the defaults and interface in \ROOT\ 5.32.  Note, \ROOT\ 5.34 provides a C++ and python interface fully interoperable with the XML interface and classes for analytically fitting bin-by-bin statistical uncertainties on the templates.  These developments will be included in a future version of this document.


\section{Preliminaries}

To understand the likelihood functions that are constructed by HistFactory, we will describe a simple example.
Consider a measurement of the rate of a single process that is performed by measuring the distribution a variable over events of an experiment that satisfy a certain, predefined criteria.
This criteria, which is a function of observables, defines the signal region.
Assume that there are two classes of events that are present in the signal region, the signal process whose properties we're interested in measuring, ``S'', and a background process, ``B''.
Denoting the observable we're studying as $x$, we begin by assuming that we know the distribution of this variable under both the signal and background processes individually.
We will refer to these distributions as $f_{\rm S}(x)$ and $f_{\rm B}(x)$, noting that, as probability distribution functions, they each individually integrate to 1.
%Typically in high energy physics, these distributions are modeled as histograms having a finite number of bins.
%If we make this assumption, then we denote $f_{\rm S}$

We will also define a parameter which scales with the expected number of events coming from the signal process in the signal region, which we refer to as the signal strength, or simply $\mu$.
And finally, we introduce a parameter, $B$, that scales with the number of background events.
We will initially assume that this parameter is known and fixed.

%Thus, given a value of the parameter $\mu$, the expected number of events in the $i

%% Let us begin by considering the simple case of a single channel with one signal and one background contribution and no systematics based on the discriminating variable is $x$.  
%% While we will not continue with this notation, let us start with the familiar convention where the number of signal events is denoted as $S$ and the number of background events as $B$.  
%% Similarly, denote the signal and background ``shapes'' as $f_{\rm S}(x)$ and $f_{\rm B}(x)$ and note the these are probability density functions normalized so that $\int dx f(x)=1$.  
%% It is common to introduce a ``signal strength'' parameter $\mu$ such that $\mu=0$ corresponds to the background-only hypothesis  and $\mu=1$ corresponds to the nominal signal+background hypothesis.  
%% This continuous parameter $\mu$ is our parameter of interest.


Thus, given a value of the parameter $\mu$, the distribution of a number of measured values of x is given by:

\begin{eqnarray}
{\cal P}(\{x_1\dots x_n\} | \mu) = \textrm{Pois}(n | \mu S+B) \left[ \prod_{e=1}^n \frac{\mu S f_{\rm S}(x_e) + B f_{\rm B}(x_e)}{\mu S+B}\right ] \; .
\end{eqnarray}

The first term on the right hand side describes the total number of events measured as a Poisson process whose mean is the sum of the expected number of signal and background events.
The remaining of the right side describes the probability of each individual event having the value $x_e$.
In total, this distribution is known as a marked Poisson model.

Often in high energy physics, the distributions $f_{\rm S}(x)$ and $f_{\rm B}(x)$ are described as histograms.
In this case, we denote the signal and background histograms as $\nu_b^{\rm sig}$ and $\nu_b^{\rm bkg}$, where $b$ is the bin index.
In the case of histograms,  marked Poisson process becomes

\begin{eqnarray}
%{\cal P}(n_b |\mu) &=&  \textrm{Pois}(n_{\rm tot} | \mu S + B) \left[\, \prod_{b\in\rm bins}\frac{\mu \nu_b^{\rm sig} + \nu_b^{\rm bkg}}{\mu S + B}\right ] \\
%&=& \mathcal{N}_{\rm comb} \prod_{b\in\rm bins} \textrm{Pois}(n_b | \mu \nu_{b}^{\rm sig} + \nu_{b}^{\rm bkg}) \; ,
{\cal P}(n_b |\mu) &=& \mathcal{N}_{\rm comb} \prod_{b\in\rm bins} \textrm{Pois}(n_b | \mu \nu_{b}^{\rm sig} + \nu_{b}^{\rm bkg}) \; ,
\label{eqn:histpois}
\end{eqnarray}
where $n_b$ is the histogram of the observed data and $\mathcal{N}_{\rm comb}$ is a constant combinatorial factor that can be neglected for the purpose of statistical analysis.   
%Similarly, we denote the histogram of the distribution of $x$ as measured in data by  $n_b$.


%% Now we ask what the probability model is for obtaining $n$ events in the data where the discriminating variable for event $e$ has a value $x_e$; thus the full dataset will be denoted $\{x_1\dots x_n\}$.  First one must include the Poisson probability of obtaining $n$ events when $\mu S + B$ are expected.  Secondly, one must take into account the probability density of obtaining $x_e$ based on the  relative mixture $f_{\rm S}(x)$ and $f_{\rm B}(x)$ for a given value of $\mu$.   Putting those two ingredients together one obtains what statisticians call a ``marked Poisson model'':
%% \begin{eqnarray}
%% {\cal P}(\{x_1\dots x_n\} | \mu)=  \textrm{Pois}(n | \mu S+B) \left[ \prod_{e=1}^n \frac{\mu S f_{\rm S}(x_e) + B f_{\rm B}(x_e)}{\mu S+B}\right ] \; .
%% \end{eqnarray}
%% If one imagines the data as being fixed, then this equation depends on $\mu$ and is called the likelihood function $L(\mu)$.  Simply taking the logarithm of the equation above and remembering that $\textrm{Pois}(n|\nu) = \nu^n e^{-\nu} / n!$ gives us a familiar formula referred to by physicists as an ``extended maximum likelihood fit'' :
%% \begin{eqnarray}\nonumber
%% -\ln L( \mu) &=&  -n \ln(\mu S + B) + (\mu S + B) + \ln n! - \sum_{e=1}^n \ln \left[ \frac{\mu S f_{\rm S}(x_e) + B f_{\rm B}(x_e)}{\mu S+B}\right] \\
%% %&=&\underbrace{ (\mu S + B)}_{\rm extended~term} - \sum_{e=1}^n \ln \left[ \mu S f_{\rm S}(x_e) + B f_{\rm B}(x_e)\right] + \underbrace{\ln n!}_{\rm irrelevant~constant} \; .
%% &=& (\mu S + B) + \ln n!- \sum_{e=1}^n \ln \left[ \mu S f_{\rm S}(x_e) + B f_{\rm B}(x_e)\right]  \; .
%% \end{eqnarray}

%% Since \HF\ is based on histograms, it is natural to think of the binned equivalent of the probability model above.  Denoted the signal and background histograms as $\nu_b^{\rm sig}$ and $\nu_b^{\rm bkg}$, where $b$ is the bin index and the histograms contents correspond to the number of events expected in the data.   We can relate the bin $\nu_b$ and the shape $f(x)$ via
%%  \begin{equation}
%%  \label{eq:shape}
%% %   S \int _{b_e} dx f_S(x_e ) = \nu_{b_e}^{\rm sig}  \hspace{.5in}  \textrm{and}  \hspace{.5in}  f_B(x_e ) = \frac{\nu_{b_e}^{\rm bkg}}{B} \;,
%%     f_S(x_e ) = \frac{\nu_{b_e}^{\rm sig}}{S \Delta_{b_e}}   \hspace{.5in}  \textrm{and}  \hspace{.5in}  f_B(x_e ) = \frac{\nu_{b_e}^{\rm bkg}}{B \Delta_{b_e}} \;,
%% \end{equation}
%% where $b_e$ is the index of the bin containing $x_e$ and $\Delta_{b_e}$ is the width of that same bins.  Note, because the $f(x)$ are normalized to unity we have $S=\sum_b  \nu_{\rm b}^{\rm sig}$ and $B=\sum_b  \nu_{\rm b}^{\rm bkg}$.


%%  Formally one can either write the probability model in terms of a product over Poisson distributions for each bin of the histogram,  or one can also continue to use the unbinned expression above recognizing that the shapes $f(x)$ look like histograms (ie. they are discontinuous at the bin boundaries and constant between them).  Technically, the \HF\ makes a model that looks more like the unbinned expression with a single RooAbsPdf that is ``extended'' with a discontinuous shape in $x$.  Nevertheless, it can be more convenient to express the model in terms of the individual bins.  Then we have
%% \begin{eqnarray}
%% {\cal P}(n_b |\mu)=  \textrm{Pois}(n_{\rm tot} | \mu S + B) \left[\, \prod_{b\in\rm bins}\frac{\mu \nu_b^{\rm sig} + \nu_b^{\rm bkg}}{\mu S + B}\right ] = \mathcal{N}_{\rm comb} \prod_{b\in\rm bins} \textrm{Pois}(n_b | \mu \nu_{b}^{\rm sig} + \nu_{b}^{\rm bkg}) \; ,
%% \end{eqnarray}
%% where $n_b$ is the data histogram and $\mathcal{N}_{\rm comb}$ is a combinatorial factor that can be neglected since it is constant.   Similarly, denote the data histogram is $n_b$.

This is a simple but powerful example of the type of probability distribution function that can be produced by HistFactory.
For a given set of observed data, it can be interpreted as a likelihood function of $\mu$, the parameter of interest.



\section{Generalizations}

HistFactory make it easy to include a number of generalizations of this problem which all one to create flexible and powerful models.
A simple generalization is the inclusion of additional search channels.
The model described above considers a single observable defined in a single signal region.
However, it is common for analyses to define several categories of events defined by different criteria for the purpose of making a measurement.
Each of these categories is known as a search channel, or simply a channel.
Within each channel, one or more discriminating variables are measured.
The total distribution for this variable (or the join distribution if multiple variables are used) is comprised of events coming from many types of physical processes, 
some of which are considered signal and some of which are background.
Each of these underlying physical processes, be it signal or background, is known in HistFactory as a sample.
A sample may appear in one, several, or all channels considered by an analysis (but only once per channel).
If it appears in more than one channel, it most likely shares a common parameter across channels.
HistFactory is designed to make this sharing of parameters conceptually simple and statistically correct.

A statistical model spanning several search channels can be created by combining likelihoods designed to describe individual search channels.
One can express the joint likelihood function of several channels as the product of the likelihood functions for individual channels (assuming that the probability of an event falling into more than one search channel is, almost surely, zero).
%But if one simultaneously studies a (potentially) different variable in a different signal region (that has no overlap with the original signal region), then one can simply express the joint likelihood function as the prodicut of the individual likelihood functions.
However, when making this product, one should be sure to use a single variable to express common parameters that are meant to be identified across likelihoods.
For example, in the joint likelihood function, one should use a single parameter $\mu$ to describe a signal strength that is coherent across search channels.
One can include any number of channels in this way.
In addition, it is easy to include multiple discriminating variables in each channel by using multidimensional histograms.
Similarly, one can consider any number of background distributions, which simply get added to equation~\ref{eqn:histpois}.

Most importantly, however, is the ability to include parameters to describe systematic uncertainties in additional to the parameter of interest.
HistFactory has developed a number ways to describe the effect of systematic uncertainties, some of which are quite intricate and sophisticated.
For the purpose of explanation, we will here only describe the two most common cases.

The two types of systematic uncertainties modeled by HistFactory that we will describe here are uncertainties that can scale the overall yield of a channel (or several channels coherently), and systematic uncertainties that can effect the shape of binned distributions in a channel (or across several channels coherently).

The first type of uncertainty is modeled by including a parameter per channel and per sample in that channel (if necessary) that scales the overall expected yield from that sample in that channel.
If several samples (either in the same channel or across multiple channels) are effected by the same logical source of systematic uncertainty, each will have a separate parameter that determines the relative size of the change in yield for that each sample individually.
However, because the source of uncertainty is common across those channels, a single, global parameter will control the size, as measured in standard deviations, of the effect, which is then propagated to each sample.
The relative size of the response in each sample to the underlying source of systematic uncertainty is an input provided by the user.
For each source of systematic uncertainty and for each sample effected by the uncertainty, the user provides the relative change in yield (where 1.0 means no change) when the parameter describing the underlying source of systematic uncertainty has a one standard deviation upward fluctuation, and likewise for a downward fluctuation of one standard deviation.
The size of the response for values other than exactly $\pm 1$ sigma are obtained as interpolations between the relative size of the $\pm 1$ sigma response sizes per channel as a function of the parameter describing the underlying source of uncertainty.
In the language of HistFactory, this type of systematic uncertainty is known as an ``OverallSys.''
Typically, a linear interpolation is used, though other schemes are available, and smoother interpolations (which don't have discontinuous derivatives) tend to perform better under gradient descent minimization schemes.

To illustrate, for a channel consisting of only 1 bin, the expected number of events in that channel is given by equation\ref{Eq:alphaInterpolation}.
\begin{eqnarray}\label{Eq:alphaInterpolation}
N^{\text{exp}}_{i, \text{tot}}(\vec{\alpha}) &=& \sum_{\text{background}} N^{\text{exp}}_{i} (1 + \sum_{j} \alpha_{j} \, \Delta N_j  ) \, \\
\Delta N_j &=& \begin{cases} \Delta N^{+}_{j}, & \mbox{if } \alpha_{j} > 0  \\ \Delta N^{-}_{j}, & \mbox{if } \alpha_{j} < 0 \end{cases},
\end{eqnarray}
where $\Delta N^{+}_{j}$ and $\Delta N^{-}_{j}$ are the differences in the expected number of events due to an upward or downward fluctuation
of one standard deviation of the $j^{th}$ nuisance parameter.

This type of systematic uncertainty may be interpreted as a parameter that describes on uncertainty on the overall number of events for either a signal or background signal.
Since it effects an overall expected yield, it is commonly used to describe an uncertainty on an efficiency, including a trigger, identification, or reconstruction efficiency, or an uncertainty on an overall cross-section (as it is commonly used in analysis describe in this thesis).

The second type of uncertainty considered is a natural extension of the ``OverallSys'' category of uncertainties, and is known as a ``HistoSys.''
A HistSys, similar to an OverallSys, is an effect that can vary per sample and per channel (including having no effect) but is controlled by a single, common parameter
that represents the underlying source of systematic uncertainty.
For every sample effected by this source of systematic uncertainty, the user supplies template histograms representing the shape of the distribution of that sample for both the plus 1-sigma and minus 1-sigma values for the global source of systematic uncertainty.
The shape of the discriminating variable for that sample for an arbitrary value of the source of systematic uncertainty comes from a continuous interpolation of those templates.

This type of effect from a systematic uncertainty can be used to describe changes in the shape of distributions that can be attributed to a source of systematic uncertainty.
A common example is the case where a kinematic variable being modeled depends on the energy of jets in the event (such as an invariant mass, or even the number of reconstructed jets).
In this case, an uncertainty on the overall jet energy scale will causes changes in the shape of this discriminating variable.

Both categories of systematic uncertainties described above use a single, global parameter to model the underlying source of that uncertainty.
When building the full likelihood function, constraint terms are added to the likelihood which describe the certainty with which these parameters are known.
Each parameter describing a source of systematic uncertainty is constrained by a Gaussian product in the likelihood.
However, it may be desirable to not include a constraint term on a source of systematic uncertainty.
In HistFactory, a ``NormFactor'' is effectively an ``OverallSys'' that doesn't include a constraint term in the likelihood.
In such a case, the parameter described by the NormFactor is said to ``float'' and is unconstrained in likelihood fits.
This is useful for modeling parameters of interests, and is used throughout the analyses described in this thesis to model the $\ttbar$ cross-section and the various cross-sections of exotic models.

\section{The likelihood function}

HistFactory's more important feature is providing a software representation of the joint likelihood across all channels and including all signals and background considered by an analysis.
This likelihood function describes the distribution of a discriminating variable (or variables) in each channel as a joint, binned distribution (in ROOT, this is supplied by an n-dimensional histogram).
It takes into account the effect of several types of systematic uncertainties, including those described above, as well as parameters of interest.
Moreover, since nearly all analyses made using the ATLAS detector depend on the value of the total integrated luminosity (which has a finite and non-negligible uncertainty),
a parameter describing the systematic uncertainty on the measured integrated luminosity is included by default.
One can turn this effect off for samples that are evaluated in data, meaning their estimates don't depend on the measured value of integrated luminosity (they of course depend on the actual value of integrated luminosity, but not measurements of that value).

The likelihood function created by HistFactory is given by equation~\ref{Eq:HistFactoryLikelihoodTemplate}.

\begin{eqnarray}
\label{Eq:HistFactoryLikelihoodTemplate}
\nonumber
{\cal P}( n_{cb}, a_p \; |\,\phi_p,\alpha_p,\gamma_b)=   \prod_{c\in \textrm{channels}} \; \prod_{b\in \textrm{bins}} \textrm{Pois}(n_{cb}|\nu_{cb}) \cdot G(L_0 | \lambda, \Delta_L) \cdot \prod_{p\in \mathbb{S}+\mathbf{\Gamma}} f_p(a_p|\alpha_p)
\end{eqnarray}

\begin{itemize}
%\item $b_e$ the bin containing $x_e$ (ie. the bin for event $e$)
\item $ \lambda_{cs}$ - luminosity parameter for a given channel and sample.  Within a given channel this parameter is a common luminosity parameter for all the samples that include luminosity uncertainty (i.e.. \texttt{NormalizeByTheory="True"}).  For all the samples with  \texttt{NormalizeByTheory="False"} it is fixed to the nominal luminosity $\lambda_{cs}=L_0$.
\item $\gamma_{cb_e}$ - Bin-by-bin scale factor used for statistical uncertainties, bin-by-bin shape systematics (\SS), and data-driven shape extrapolations (\SF).  For statistical errors, the $\gamma_{csb_e}$ is shared for all the samples in the channel (ie. subscript $s$ can be omitted).  For samples that do not have any bin-by-bin scale factors $\gamma_{csb_e}=1$.
\item $\alpha_p$ - Parameters associated to sources of systematic uncertainty that have constraints
\item $f_p(a_p|\alpha_p)$ - A constraint term describing an auxiliary measurement $a_p$ that constrains a nuisance parameter $\alpha_p$
\item $ \phi_{cs}$ - Product of unconstrained normalization factors for a given sample within a given channel.  These typically include the parameter of interest, eg. the signal cross-section or branching ratio.
 \begin{equation}
 \phi_{cs}= \prod_{p\in\mathbb{N}_c} \phi_p
 \end{equation}
\item $\eta_{cs}(\vec\alpha)$  - The parametrized normalization uncertainties (ie. \OS) for a given sample within a given channel (a factor around 1).
\item $\sigma_{csb_e}$  - The parametrized histogram (ie. the nominal histogram and the \HS) for a given sample within a given channel.
\end{itemize}


%% \section{Statistical Uncertainties}
%% Histfactory uses shape templates to model the distributions of signal and background samples.
%% These templates are typically derived using Monte Carlo simulation or by measuring a distribution
%% in a control region.
%% Both of these methods use training events to construct a distribution.
%% When this number is small, the effect of having finite statistics to construct a
%% shape template may become important.
%% To address this issue, HistFactory introduced a sceme that paramaterizes changes in the shapes
%% of distribution templates.

