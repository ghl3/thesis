

\section{Statistics}

%Properly using statistical tools ... blah blah blah

Experimental High Energy Physics is uniquely suited for precise, reliable, and accurate statistical analysis.
The detailed knowledge of the underlying physics allow one to create extremely accurate probabilistic models and detailed simulations of measureable physical phenomena.
Large datasets allow these models to be checked, calibrated, and finely tuned to better describe the underlying physics.
Sufficient computational resources are available for massive amounts of simulation to be generated from different models or similar models with various sets of parameters.
Finally, there are a plethora of statistical formulae, tools, and techniques dedicated to the types of measurements performed by High Energy Experimental Physicists.
The above advantages make ATLAS a nearly ideal environment for detailed statistical analysis.

%ATLAS uses a variety of standard statistical techniques for performing measurements.
%The standard set at ATLAS is to use ``frequentist'' techniques, though this is an extremely loaded term.
%Perhaps it is better to say that ATLAS uses ``likelihood'' techniques (a term which is both more specific and leads to less controversy).
%In practice, this means that measurements are made by constructing a likelihood function, which is a real valued function, $L(data | \vec{\alpha})$, whose value is ves the probability of a data point or a set of data points as a function of a number of parameters, $\vec{\alpha}$.

The majority of measurements performed by the ATLAS collaboration use frequentist techniques (or approximate frequentist techniques).
However, both frequentist and bayesian techniques require that physical observables be properly modeled and described by a likelihood function.

A model is described in terms of its probability density function, or ``pdf'', which returns the probability of generating a measurable value or a set of measurable values, known collectively as the ``data''.
Because it is a function of data that returns a probability, it must be normalized to unity over the space of datasets such that
\begin{equation}
\int pdf(\vec{D}) d\vec{D} = 1
\end{equation}

%A probability density function, or ``pdf'', is a function of a measured value or a set of measured values, known collectively as the ``data'', which evaluates to the probabilty density of a particular model generating that data.
Often, one describes a family of similar pdf's by paramaterizing the density as a function of one or more parameters, which we will refer to as $\theta$.
These parameters may include parameters that one wants to measure, collectively known as the parameter(s) of interested and here referred to as $\mu$, and it may also include additional parameters, known as nuisance parameters and referred to as $\nu$. %writing the density as a function of one or more parameters.
Every pdf in this paramaterized set much individually be normalized over the space of data. 
\begin{equation}
  \int pdf(\vec{x}, \theta) d\vec{x} = 1 \hspace{5mm} \forall \theta %\ele \{\theta\}
\end{equation}

%Each individual pdf, , given a family of pdf's that are paramaterized by a real value $\alpha$, each pdf must obey:

A likelihood is a real valued function, $L(data | \vec{\theta})$, whose value is the probability of a point in data space as a function of a number of parameters, $\vec{\theta}$.
While a pdf is required to be normalized over the set of data, a likelihood is not normalized the space of parameters.
%Though they appear similar, likelihoods and pdf's are quite different objects.
%A pdf is required to be strictly normalized over the set of observable data such that:
%This must be true of any given pdf, including every pdf in a paramaterized family of functions.


In contrast, likelihoods are functions of parameters, but aren't normalized over those parameters.
Likelihoods are always associated with data, and they describe how the probability of that data changes as a function of parameters.

%The reason for this difference becomes clear if one interprets a likelihood as a functional of a pdf which maps that pdf onto a dataset to produce a real (positive) number:
One can interpret a likelihood as a functional that maps a probability distribution function, for a given dataset, to a real number between 0 and 1:
\begin{equation}
  L: \text{ \{pdf, data\} } \rightarrow % \[0, 1\] \ele \mathbb{R}
\end{equation}

%Likelihoods only become useful objects when they are manipulated to produce statistical statements.
%The type of statement that can be made from likelihoods is the source of disagreement between the ``bayesian'' and ``frequentist'' statistical sects.
%We will here take an agnostic position on the philosophical merits of any type of statistical statement, and instead present a common type of statistical statement, used and recommended by the ATLAS collaboration, as well as many other experiments, known as the confidence interval.

A likelihood is the common starting point for frequentist statistical statements (bayesian methods require the use of one or more ``prior'' pdfs in order to make statistical statements).
The two types of statistical statements that we will describe here are the ``confidence interval'' and ``limit setting.''
%These two quantities are interrelated to one another.

\subsection{Confidence Interval}

A confidence interval is a procedure that produces a set of parameter points that are compatable with measured data in a very specific, well-defined sense.
Each confidence interval is a function of the level of compatability, here referred to as $\alpha$, which is determined before the interval is created.
An instance confidence interval is a set obtained using measured data, and in a frequentist sense, any given confidence interval either contains the true parameter points or it doesn't.
However, when the procedure for producing a confidence interval is applied over many (hypothetical) realizations of measured data, the true values of the parameters will be contained in the confidence interval at a rate of $\alpha$.

%A confidence inverval is a procedure that maps a likelihood to a set in the space of parameters which describe the pdf domain of the likelihood function.
%This procedure is useful because it has the following property: given many randomly-generated realizations of the dataset described by the likelihood, the set obtained using the confidence interval procedure will contain the true value of a parameter (the one which describes the true pdf) a fixed percentage of the time, where that percentage is known in advance and determines the procedure for obtaining confidence intervals.

%In other words, assuming that we knew the true distribution for a dataset that we're interested in, the following procedure is possible:
The meaning of a confidence interval can be understood as the result of the following abstract procedure:
\begin{itemize}
  \item Assume there exists true pdf, $pdf_{true} = pdf(Data | \vec{\alpha}_{true})$ 
    %, and assume that a single dataset $D_{0}$ was drawn randomly using that pdf. %which describes how a single realization of a dataset was generated
  \item Create a likelihood function for that pdf over the full space of parameter values: $L(D | \theta)$.
    % where the parameters onsidering the full family of pdf's ${pdf(Data | \theta) \forall \theta \ele {\theta}}$, create a likelihood function evaluated on the measured data: 
    % \item Create a likelihood function over the space of pdf's and datasets, assuming that the true pdf is contained in the domain of this likelihood function
  \item Choose a confidence level, $\alpha$, which must be a value between 0 and 1.
  \item Using the true pdf, generate many datasets, and for each dataset, create a confidence interval of size $\alpha$. %set in parameter space $S_i$ based on the above procedure.
%  \item For this given confidence level, and using the measured likelihood, create a confidence interval, which is a set $S_i$ in the space of parameters $\vec{\theta}$
    %  \item There exists a specific procedure, based on that chosen confidence level, which produces a set $S_i$ in the space of parameters $\vec{\alpha}$ using our likelihood function, which we denote as the ``confidence interval''.  
  \item Given this set of intervals, {$S_i$}, count the fraction which contain $\alpha_{true}$.
  \item As the number of generated datasets approaches infinity, this fraction will approach the confidence level, $\alpha$.
\end{itemize}

Confidence intervals are only useful if one finds the above property useful.
Stricly speaking, a confidence interval alone can not be used to evaluate the value or utility of a decision based on observed data.
They are, however, objective means of quantifying data, in the sense that they don't depend on assumptions about parameters other than those that are build into the likelihood function.
%Any other way of arguing for their merit or usefulness is strictly philosophical.
%We will here assume that confidence intervals are a desirable quantity to calculate and will show how they can be obtained for likelihoods that describe data obtained using the ATLAS detector.

\subsection{Hypothesis Testing}
A typical problem addressed by high energy physics is the comparison of two probability models that are each generated from different physical processes.
Specifically, the two main types of hypothesis tests used at the LHC are for declaring descovery or setting limits.
These two types of tests are applications of a common frequentists procedure to select between potential probabilistic models.
% High Energy Physics at the LHC follows the perscriptions defined by Neyman and Pearson.
Their simple perscription begins by dividing the models being compared into categories of ``Null'' and ``Alternate.'' 
At the LHC, the ``Null'' hypotheis is associated with the Standard Model, and the ``Alternate'' hypothesis represents some new physical process that one is searching for (in the case of the Higgs Boson, the presence of a Higgs is considered an ``Alternate'' hypothesis, even though, in a strict sense, it is necessary for the Standard Model to be mathematically consistent).
One then decides upon a threshold for rejecting the background hypothesis, known as the size of the test, or the probability of a Type-I error, or simply as $\alpha$.
This must be chosen by the experimenter and is based on one's willingness to falsely reject the NULL hypothesis (there are commonly accepted values for this number, but any particular choice of value is purely philosophical).

Given a choice of $\alpha$, one then chooses a region in the space of possible measured data with a total probability measure under the Null hypothesis of size $\alpha$.
We will denote this region as $K_{\alpha}$, emphacizing the fact that the size of the region is determined by the chosen value $\alpha$.
This 
Clearly, there are many such regions (often infinitely many), and the task of choosing an arbitrary volume in data space may seem daunting.
Typically, this region is selected by first choosing a ``test statistic,'' which is a mapping of the space of data to the real number line.
With such a function in hand, the task then becomes picking a 1-dimensional region in the range of the Test Statistic such that $ P( T(d) in K_{ \alpha } | NULL) = \alpha$ (typically, this region is determined by picking a cut-off value, $k_{\alpha}$, such that $K_{\alpha} = \{d : T(d) > k_{\alpha} \}$).
Given a region $K_{a}$, the ``power'' of this region is defined to be $1 - \beta$, where $\beta = P(D element K_{ \alpha } | Alternate)$ and is commonly known as the probability of a Type-II error.
For a chosen size $\alpha$, an ideal test maximizes $\beta$.
In practice, this task boils down to choosing the most powerful test statistic, from which the value of $k_{\alpha}$ and the region in data $K_{\alpha}$ are determined (though, it may take some analytic or computational to determine the value of $k_{\alpha}$).

In the case of comparing two probability models without any parameters (or equivalantly, comparing models with fixed parameters), a result known as the Neyman-Pearson lemma states that the most powerful test statistic is the likelihood ratio, defined as the ratio of the likelihoods of data under the two models being compared.
Although this result is powerful, its usefulness is often limited due to the fact that the presence of systematic uncertainties requires models to be paramaterized.

The presence of systematic uncertainties makes the determination of the region $K_{\alpha}$ more complicated.
One convention is to choose $K_{\alpha}$ such that the supremum of the measure of probability in that region under all possible Null models over the space of parameters is $\alpha$.
In other words, $\alpha = max( \int P(D|\theta) d(D) )$ where the maximum runs over all values of $\theta$.
If the alternate hypothesis is also paramaterized, then the power of this test will no longer be well defined, but rather it will depend on the values of the parameters.


The symmetry between these regions is broken by the alternate hypothesis.
Given a fixed region


\subsection{Neyman Construction}

A standard way of conceptualizing and constructing confidence intervals is through the use of a ``Neyman Construction.'' 

%% http://inspirehep.net/record/582577
%% @article{Conrad:2002kn,
%%       author         = ``Conrad, Jan and Botner, O. and Hallgren, A. and Perez de los Heros, Carlos'',
%%       title          = ``{Including systematic uncertainties in confidence
%%                         interval construction for Poisson statistics}'',
%%       journal        = ``Phys.Rev.'',
%%       volume         = ``D67'',
%%       pages          = ``012002'',
%%       doi            = ``10.1103/PhysRevD.67.012002'',
%%       year           = ``2003'',
%%       eprint         = ``hep-ex/0202013'',
%%       archivePrefix  = ``arXiv'',
%%       primaryClass   = ``hep-ex'',
%%       SLACcitation   = ``%%CITATION = HEP-EX/0202013;%%'',
%% }

%% http://inspirehep.net/record/454197
%% @article{Feldman:1997qc,
%%       author         = ``Feldman, Gary J. and Cousins, Robert D.'',
%%       title          = ``{A Unified approach to the classical statistical analysis
%%                         of small signals}'',
%%       journal        = ``Phys.Rev.'',
%%       volume         = ``D57'',
%%       pages          = ``3873-3889'',
%%       doi            = ``10.1103/PhysRevD.57.3873'',
%%       year           = ``1998'',
%%       eprint         = ``physics/9711021'',
%%       archivePrefix  = ``arXiv'',
%%       primaryClass   = ``physics.data-an'',
%%       reportNumber   = ``HUTP-97-A096'',
%%       SLACcitation   = ``%%CITATION = PHYSICS/9711021;%%'',
%% }

%% http://inspirehep.net/record/1081445
%% @article{Rolke:2011na,
%%       author         = ``Rolke, Wolfgang A. and Lopez, Angel M.'',
%%       title          = ``{Estimating a Signal In the Presence of an Unknown Background}'',
%%       journal        = ``Nucl.Instrum.Meth.'',
%%       volume         = ``A685'',
%%       pages          = ``16-21'',
%%       doi            = ``10.1016/j.nima.2012.05.029'',
%%       year           = ``2012'',
%%       eprint         = ``1112.2299'',
%%       archivePrefix  = ``arXiv'',
%%       primaryClass   = ``physics.data-an'',
%%       SLACcitation   = ``%%CITATION = ARXIV:1112.2299;%%'',
%% }

A Neyman Construction is a set of points in the set of parameters and possible measured data, as well as a procedure for using those points to obtain confidence intervals.
The construction is build as follows:
Consider a likelihood function over a space of parameters, $\vec{\theta}$, and over a space of measured data, $\vec{d}$: $L = L(\vec{d} | \vec{\theta} )$.
Pick a value $\alpha$ which represents the size (strength) of the confidence interval to be built.  This must be agreed upon in advance.
Then, for each point in parameter space, $\vec{ \theta }$ (which defines a fixed likelihood function), create an a region in the set of data, ${ \vec{d} }$, known as an ``acceptance region'', which is constructed such that the total probability measure of that region is $\alpha$: ${ \vec{d}_{ \vec{ \theta } } : P( \vec{d} element \vec{d}_{ \vec{ \theta } } | \vec{ \theta } ) = \alpha }$.
This requires knowing, or at least being able to integrate, the likelihood of data at a fixed point $\vec{\theta}$ (this distribution can be obtained using Monte-Carlo techniques once $\vec{\alpha}$ has been specified).
Of course, there are many (often infinitely many) such regions for a given parameter point.
Therefore, one must decide in advance how an acceptance region is to be constructed given a fixed likelihood function.
Often, this decision is made by creating an ordering rule, which determines the order in which points are used to populate a the region, which is then filled in order until it has total measure $\alpha$.

An acceptance region is constructed for all possible values of $\vec{\theta}$.
The union of every such region (including both the data and the parameter points) is known as the confidence belt: confidence belt = $ union ( \vec{d}, \vec{ \theta }) : ( \vec{d}, \vec{ \theta }) element $ some acceptance region for some $\vec{ \theta } $.
%(the name comes from the cases where there is 1 parameter and 1 data value in the likelihood; in more complicated situations, it is better described as a volume or a multidimensional set).
%We do this for all values of theta to define a belt (or, if theta is a vector, we define a volume in parameter space).

Using this set, a confidence region for a given measured data point, $\vec{d}_0$, can be constructed as the union of all points in the confidence belt which include the measured data point: ${ ( \vec{ \theta }, \vec{d}) element belt : d = d_{0}}$.
Specifically, confidence region for a particular parameter, $\alpha$, is union of all values of alpha in the confidence belt that contain $\vec{d}_0$.

If $\theta_0$ were the true value, all the data would fall in the interval in defined by $\theta_0$ $\alpha$ percent of the time.
In addition, if the data were to fall in the confidence region of $\theta_0$, then the interval of parameter points would include $\theta_0$.
These two imply the statement: $\theta_0$ is in the confidence interval $\iff$ the data falls in the region of the likelihood function L($\theta_0$).
Thus, the coverage of the region is identical to the coverage of the interval, which is $\alpha$.
Therefore, the confidence interval defined by the above construction is guaranteed proper coverage.

In this sense, a confidence interval is the inversion of a hypothesis.
It represents the set of all points in parameter space for which the measured data falls within an acceptance region of size $\alpha$ (given some ordering rule).
For any single parameter in parameter space, the confidence interval is the the union of all points of that parameter that fall in the confidence band (confidence volume) in the full parameter space.


\subsection{Hypothesis Testing}
% Kyle:  Said yet another way, to claim a discovery, the confidence interval for the nuisance parameter(s) must be empty (when the construction is made assuming the null hypothesis).

\subsection{Systematic Uncertainties}



\subsection{Profile Likelihood}


