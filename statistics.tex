

\section{Statistics}

%Properly using statistical tools ... blah blah blah
Experimental High Energy Physics is uniquely suited for precise, reliable, and accurate statistical analysis.
The detailed knowledge of the underlying physics allow one to create accurate probabilistic models and detailed simulations of measurable physical phenomena.
Large datasets allow these models to be checked, calibrated, and finely tuned to better describe the underlying physics.
Sufficient computational resources are available for massive amounts of simulation to be generated from different models or similar models with various sets of parameters.
Finally, there are a plethora of statistical formulae, tools, and techniques dedicated to the types of measurements performed by High Energy Experimental Physicists.
The above advantages make ATLAS a nearly ideal environment for detailed statistical analysis.

%ATLAS uses a variety of standard statistical techniques for performing measurements.
%The standard set at ATLAS is to use ``frequentist'' techniques, though this is an extremely loaded term.
%Perhaps it is better to say that ATLAS uses ``likelihood'' techniques (a term which is both more specific and leads to less controversy).
%In practice, this means that measurements are made by constructing a likelihood function, which is a real valued function, $L(data | \vec{\alpha})$, whose value is ves the probability of a data point or a set of data points as a function of a number of parameters, $\vec{\alpha}$.

In high energy physics, the process of performing a statistical analysis consists of two separate steps: the modeling of data and the act of measurement using that model.
The first step requires understanding the underlying theory behind a data, the experimental setup that produced the data, and sources of uncertainty or noise that are present in the data.
The second part requires determining what parameters on is interesting in measuring, the types of questions one would like to answer with those measurements, and what mathematical techniques or approximations one is willing to make when making those measurements.


\subsection{Likelihood}
A physical, mathematical, or statistical model of data requires the construction of a likelihood, which is a function that returns the probability of a given dataset under a specified model.
A model is described in terms of its probability density function, or ``pdf'', which returns the probability of generating a measurable value or a set of measurable values, known collectively as the ``data''.
Because it is a function of data that returns a probability, it must be normalized to unity over the space of datasets such that
\begin{equation}
\int pdf(\vec{D}) d\vec{D} = 1
\end{equation}

%A probability density function, or ``pdf'', is a function of a measured value or a set of measured values, known collectively as the ``data'', which evaluates to the probabilty density of a particular model generating that data.
Often, one describes a family of similar pdf's by paramaterizing the density as a function of one or more parameters, which we will refer to as $\theta$.
These parameters may include parameters that one wants to measure, collectively known as the parameter(s) of interested and here referred to as $\mu$, and it may also include additional parameters, known as nuisance parameters and referred to as $\nu$. %writing the density as a function of one or more parameters.
Every pdf in this paramaterized set much individually be normalized over the space of data. 
\begin{equation}
  \int pdf(\vec{x}, \theta) d\vec{x} = 1 \hspace{5mm} \forall \theta %\in \{\theta\}
\end{equation}
%Each individual pdf, , given a family of pdf's that are paramaterized by a real value $\alpha$, each pdf must obey:

A likelihood is a real valued function, $L(data | \vec{\theta})$, whose value is the probability of a point in data space as a function of a number of parameters, $\vec{\theta}$.
While a pdf is required to be normalized over the set of data, a likelihood is not normalized the space of parameters.
%Though they appear similar, likelihoods and pdf's are quite different objects.
%A pdf is required to be strictly normalized over the set of observable data such that:
%This must be true of any given pdf, including every pdf in a paramaterized family of functions.
In contrast, likelihoods are functions of parameters, but aren't normalized over those parameters.
Likelihoods are always associated with data, and they describe how the probability of that data changes as a function of parameters.

%The reason for this difference becomes clear if one interprets a likelihood as a functional of a pdf which maps that pdf onto a dataset to produce a real (positive) number:
One can interpret a likelihood as a functional that maps a probability distribution function, for a given dataset, to a real number:
\begin{equation}
  L: \text{ \{pdf, data\} } \rightarrow [0, 1]% \[0, 1\] \in \mathbb{R}
\end{equation}

%Likelihoods only become useful objects when they are manipulated to produce statistical statements.
%The type of statement that can be made from likelihoods is the source of disagreement between the ``bayesian'' and ``frequentist'' statistical sects.
%We will here take an agnostic position on the philosophical merits of any type of statistical statement, and instead present a common type of statistical statement, used and recommended by the ATLAS collaboration, as well as many other experiments, known as the confidence interval.

%\subsection{Building Likelihoods at ATLAS}

The creation of a likelihood function is often the most important step in an analysis of physical data.
It encapsulates (hopefully) the entirety of one's knowledge about an experiment, including important parameters that one wants to study and theoretical or experimental uncertainties that effect those parameters.
Specific techniques for creating likelihood functions to describe the data measured by the ATLAS experiment are described in section ~\ref{app:histfactory}.

A likelihood serves as the common starting point for statistical techniques (many statistical techniques don't explicitly require a likelihood function, but they often imply the presence of a particular likelihood function).
%frequentist statistical statements (bayesian methods require the use of one or more ``prior'' pdfs in order to make statistical statements).
The majority of statistical measurements performed by the ATLAS collaboration use frequentist techniques (or approximations to strict frequentist techniques).
However, both frequentist and Bayesian techniques require that physical observables be properly modeled and described by a likelihood function.
In the following sections, we will review the concept of hypothesis testing as applied to the ATLAS experiment.
We will then describe two main categories of statistical measurements known as ``confidence intervals'' and ``limit setting.''
%These two quantities are interrelated to one another.


\subsection{Hypothesis Testing}
A typical problem addressed by high energy physics is the comparison of two probability models that describe different underlying physical processes.
At ATLAS, this often means comparing a new physical theory to the Standard Model for the purpose of either claiming discovery or setting limits.
%These two types of tests are applications of a common frequentists procedure to select between potential probabilistic models.
% High Energy Physics at the LHC follows the perscriptions defined by Neyman and Pearson.
In a more general case, one considers two separate physical models, each with their own likelihood functions that describe the same data, which we will refer to as the ``Null'' hypothesis and the ``Alternate'' hypothesis.
At the LHC, the ``Null'' hypothesis is often the Standard Model, and an ``Alternate'' hypothesis may be a new physical process that one is searching for (in the case of the Higgs Boson, the presence of a Higgs is considered an ``Alternate'' hypothesis, even though, in a strict sense, it is necessary for the Standard Model to be mathematically consistent).

The general procedure for performing a hypothesis is to accept one hypothesis or another based on the outcome of an experiment.
Formally, this means considering the full space spanned by all possible outcomes of an experiment and dividing that space into two segments.
If the outcome of the actual experiment performed falls into one part of the space, one chooses to accept the corresponding hypothesis, and if it falls into the other, then one chooses to believe the other hypothesis.
The determination of the boundary between these regions and the probability measure of the two regions is the core challenge of setting up a hypothesis test.

In frequentist hypothesis testing, one treats the two hypotheses being tested in an asymmetric manor.
One first decides upon a threshold for rejecting the background hypothesis, known as the size of the test, or the probability of a Type-I error, or simply as $\alpha$.
This must be chosen by the experimenter and is based on one's willingness to falsely reject the NULL hypothesis (there are commonly accepted values for this number, but any particular choice of value is purely philosophical).
In high energy experimental physics, one often chooses an extremely small value for $\alpha$ to signify that one requires an extremely high degree of confidence to reject commonly accepted physical models, such as the Standard Model.

The choice of $\alpha$ determines the size, or volume, or measure of the space out experimental outcomes that will result in the alternate hypothesis being accepted (and therefore rejecting the null hypothesis).
Given a choice of $\alpha$, one then chooses a region in the space of possible measured data with a total probability measure under the Null hypothesis of size $\alpha$.
We will denote this region as $K_{\alpha}$, emphasizing the fact that the size of the region is determined by the chosen value $\alpha$.

Clearly, there are many such regions (often infinitely many), and the task of choosing an arbitrary volume in data space may seem daunting, as the space of all experimental outcomes can be both conceptually and computationally massive.
Typically, this region is selected by first choosing a ``test statistic,'' which is a mapping of the full space of experimental outcomes to a smaller dimensional space, which is usually the real number line.
With such a function in hand, the task then becomes picking a 1-dimensional region in the range of the Test Statistic such that $P( T(d) \in K_{ \alpha } | NULL) = \alpha$ (typically, this region is determined by picking a cut-off value, $k_{\alpha}$, such that $K_{\alpha} = \{d : T(d) > k_{\alpha} \}$).
Given a region $K_{a}$, the ``power'' of this region is defined to be $1 - \beta$, where $\beta = P(D \in K_{ \alpha } | Alternate)$ and is commonly known as the probability of a Type-II error.
For a chosen size $\alpha$, an ideal test maximizes $\beta$, and better tests have higher values of $\beta$.
In practice, this task boils down to choosing the most powerful test statistic, from which the value of $k_{\alpha}$ and the region in data $K_{\alpha}$ are determined (though, it may take some analytic or computational to determine the value of $k_{\alpha}$).

In the case of comparing two probability models without any parameters (or equivalently, comparing models with fixed parameters), a result known as the Neyman-Pearson lemma states that the most powerful test statistic is the likelihood ratio, defined as the ratio of the likelihoods of data under the two models being compared.
Although this result is powerful, its usefulness is often limited due to the fact that the presence of systematic uncertainties requires one to compare not just two but rather a continuous families of models to each other.
Often, both the null and the alternate hypothesis are parameterized by variables describing a continuum of experimental or theoretical effects.

The presence of systematic uncertainties makes the determination of the region $K_{\alpha}$ more complicated.
One convention is to choose $K_{\alpha}$ such that the supremum of the measure of probability in that region under all possible Null models over the space of parameters is $\alpha$.
In other words, $\alpha = max( \int P(D|\theta) d(D) )$ where the maximum runs over all values of $\theta$.
If the alternate hypothesis is also parameterized, then the power of this test will no longer be well defined, but rather it will depend on the values of the parameters.


\subsection{Confidence Interval}

A common application of frequentist hypothesis testing techniques is the creation of confidence intervals.
A confidence interval is a procedure that produces a set of parameter points that are compatible with measured data, where compatible  in a very specific, well-defined sense.
Each confidence interval is a function of the level of compatibility, here referred to as $\alpha$, which is determined before the interval is created.
An instance confidence interval is a set obtained using measured data, and in a frequentist sense, any given confidence interval either contains the true parameter points or it doesn't.
However, when the procedure for producing a confidence interval is applied over many (hypothetical) realizations of measured data, the true values of the parameters will be contained in the confidence interval at a rate of $\alpha$.

%A confidence inverval is a procedure that maps a likelihood to a set in the space of parameters which describe the pdf domain of the likelihood function.
%This procedure is useful because it has the following property: given many randomly-generated realizations of the dataset described by the likelihood, the set obtained using the confidence interval procedure will contain the true value of a parameter (the one which describes the true pdf) a fixed percentage of the time, where that percentage is known in advance and determines the procedure for obtaining confidence intervals.

%In other words, assuming that we knew the true distribution for a dataset that we're interested in, the following procedure is possible:
The meaning of a confidence interval can be understood as the result of the following abstract procedure:
\begin{itemize}
  \item Assume there exists true pdf, $pdf_{true} = pdf(Data | \vec{\alpha}_{true})$
    %, and assume that a single dataset $D_{0}$ was drawn randomly using that pdf. %which describes how a single realization of a dataset was generated
  \item Create a likelihood function for that pdf over the full space of parameter values: $L(D | \theta)$.
    % where the parameters onsidering the full family of pdf's ${pdf(Data | \theta) \forall \theta \in {\theta}}$, create a likelihood function evaluated on the measured data:
    % \item Create a likelihood function over the space of pdf's and datasets, assuming that the true pdf is contained in the domain of this likelihood function
  \item Choose a confidence level, $\alpha$, which must be a value between 0 and 1.
  \item Using the true pdf, generate many datasets, and for each dataset, create a confidence interval of size $\alpha$. %set in parameter space $S_i$ based on the above procedure.
%  \item For this given confidence level, and using the measured likelihood, create a confidence interval, which is a set $S_i$ in the space of parameters $\vec{\theta}$
    %  \item There exists a specific procedure, based on that chosen confidence level, which produces a set $S_i$ in the space of parameters $\vec{\alpha}$ using our likelihood function, which we denote as the ``confidence interval''.
  \item Given this set of intervals, {$S_i$}, count the fraction which contain $\alpha_{true}$.
  \item As the number of generated datasets approaches infinity, this fraction will approach the confidence level, $\alpha$.
\end{itemize}

Often, confidence intervals are used to convey the magnitude of uncertainty of a parameter given an experiment.
But a confidence interval should not be confused with results that attempt to quantify the probability distribution of a parameter based on an experimental outcome.
A confidence interval is nothing more than a set defined in the above way.
They are useful only as far as one finds such sets useful, and a confidence interval alone can not be used to evaluate the value or utility of a decision based on observed data.
%They are, however, objective means of quantifying data, in the sense that they don't depend on assumptions about parameters other than those that are build into the likelihood function.
%Any other way of arguing for their merit or usefulness is strictly philosophical.
%We will here assume that confidence intervals are a desirable quantity to calculate and will show how they can be obtained for likelihoods that describe data obtained using the ATLAS detector.


\subsection{Neyman Construction}

The actual determination of a confidence interval is often both conceptually and computationally challenging.
A standard way of conceptualizing and constructing confidence intervals is through the use of a ``Neyman Construction.''


%% http://inspirehep.net/record/582577
%% @article{Conrad:2002kn,
%%       author         = ``Conrad, Jan and Botner, O. and Hallgren, A. and Perez de los Heros, Carlos'',
%%       title          = ``{Including systematic uncertainties in confidence
%%                         interval construction for Poisson statistics}'',
%%       journal        = ``Phys.Rev.'',
%%       volume         = ``D67'',
%%       pages          = ``012002'',
%%       doi            = ``10.1103/PhysRevD.67.012002'',
%%       year           = ``2003'',
%%       eprint         = ``hep-ex/0202013'',
%%       archivePrefix  = ``arXiv'',
%%       primaryClass   = ``hep-ex'',
%%       SLACcitation   = ``%%CITATION = HEP-EX/0202013;%%'',
%% }

%% http://inspirehep.net/record/454197
%% @article{Feldman:1997qc,
%%       author         = ``Feldman, Gary J. and Cousins, Robert D.'',
%%       title          = ``{A Unified approach to the classical statistical analysis
%%                         of small signals}'',
%%       journal        = ``Phys.Rev.'',
%%       volume         = ``D57'',
%%       pages          = ``3873-3889'',
%%       doi            = ``10.1103/PhysRevD.57.3873'',
%%       year           = ``1998'',
%%       eprint         = ``physics/9711021'',
%%       archivePrefix  = ``arXiv'',
%%       primaryClass   = ``physics.data-an'',
%%       reportNumber   = ``HUTP-97-A096'',
%%       SLACcitation   = ``%%CITATION = PHYSICS/9711021;%%'',
%% }

%% http://inspirehep.net/record/1081445
%% @article{Rolke:2011na,
%%       author         = ``Rolke, Wolfgang A. and Lopez, Angel M.'',
%%       title          = ``{Estimating a Signal In the Presence of an Unknown Background}'',
%%       journal        = ``Nucl.Instrum.Meth.'',
%%       volume         = ``A685'',
%%       pages          = ``16-21'',
%%       doi            = ``10.1016/j.nima.2012.05.029'',
%%       year           = ``2012'',
%%       eprint         = ``1112.2299'',
%%       archivePrefix  = ``arXiv'',
%%       primaryClass   = ``physics.data-an'',
%%       SLACcitation   = ``%%CITATION = ARXIV:1112.2299;%%'',
%% }

A Neyman Construction is created by considering the set of points in a space consisting of the product of all experimental results and parameter value.
The Neyman construction is performed by creating sets in this space which are used to construct confidence intervals over parameters for all possible observable realizations of an experiment.
% A Neyman Construction is a set of points in the set of parameters and possible measured data, as well as a procedure for using those points to obtain confidence intervals.
The construction is build as follows:
Consider a likelihood function over a space of parameters, $\vec{\theta}$, and over a space of measured data, $\vec{d}$: $L = L(\vec{d} | \vec{\theta} )$.
Pick a value $\alpha$ which represents the size (strength) of the confidence interval to be built.
As is standard in frequentist hypothesis testing, this must be agreed upon in advance.
Then, for each point in parameter space, $\vec{ \theta }$ (recall that there is a 1-to-1 correspondence between likelihood functions and points in parameter space), create an a region over the set of data, ${ \vec{d} }$, known as an ``acceptance region'', which is constructed such that the total probability measure of that region is $\alpha$: ${ \vec{d}_{ \vec{ \theta } } : P( \vec{d} \in \vec{d}_{ \vec{ \theta } } | \vec{ \theta } ) = \alpha }$.
This requires knowing, or at least being able to integrate, the likelihood of data at a fixed point $\vec{\theta}$ (this distribution can be obtained using Monte-Carlo techniques once $\vec{\alpha}$ has been specified).
Of course, there are many (often infinitely many) such regions for a given parameter point.
Therefore, one must decide in advance how an acceptance region is to be constructed given a fixed likelihood function.
Often, this decision is made by creating an ordering rule, which determines the order in which points are used to populate a the region, which is then filled in order until it has total measure $\alpha$.

An acceptance region is constructed for all possible values of $\vec{\theta}$.
The union of every such region (including both the data and the parameter points) is known as the confidence belt: confidence belt = ${ ( \vec{d}, \vec{ \theta }) : ( \vec{d}, \vec{ \theta }) \in \mbox{some acceptance region for some} \vec{ \theta } }$.
%(the name comes from the cases where there is 1 parameter and 1 data value in the likelihood; in more complicated situations, it is better described as a volume or a multidimensional set).
%We do this for all values of theta to define a belt (or, if theta is a vector, we define a volume in parameter space).

Using this set, a confidence region for a given measured data point, $\vec{d}_0$, can be constructed as the union of all points in the confidence belt which include the measured data point: ${ ( \vec{ \theta }, \vec{d}) \in belt : d = d_{0}}$.
Specifically, confidence region for a particular parameter, $\alpha$, is union of all values of alpha in the confidence belt that contain $\vec{d}_0$.

If $\theta_0$ were the true value, all the data would fall in the interval in defined by $\theta_0$ $\alpha$ percent of the time.
In addition, if the data were to fall in the confidence region of $\theta_0$, then the interval of parameter points would include $\theta_0$.
These two imply the statement: $\theta_0$ is in the confidence interval $\iff$ the data falls in the region of the likelihood function L($\theta_0$).
Thus, the coverage of the region is identical to the coverage of the interval, which is $\alpha$.
Therefore, the confidence interval defined by the above construction is guaranteed proper coverage.

In this sense, a confidence interval is the inversion of a hypothesis.
It represents the set of all points in parameter space for which the measured data falls within an acceptance region of size $\alpha$ (given some ordering rule).
For any single parameter in parameter space, the confidence interval is the the union of all points of that parameter that fall in the confidence band (confidence volume) in the full parameter space.

% Kyle:  Said yet another way, to claim a discovery, the confidence interval for the nuisance parameter(s) must be empty (when the construction is made assuming the null hypothesis).

\subsection{Profile Likelihood}

The above procedure is often conceptualized in the case of a 1-dimensional parameter and a single test statistic.
In this case, the confidence belt represents a 1-d region that consists of confidence intervals for every possible value of the test statistic.
However, when one considers a likelihood consisting of numerous nuisance parameters, constructing a single confidence interval for a parameter of interest is no longer well defined.
For each point in the space of nuisance parameters, a different confidence belt is obtained for a single test statistic and a single parameter of interest.

One may desire to construct a confidence interval consisting of the union of all points of that parameter that fall in the confidence band (confidence volume) in the full parameter space.
However, this is usually incredibly difficult to calculate computationally.
Instead, many results at the ATLAS experiment are calculated using an approximation to the above confidence interval using what is known as the ``Profile Likelihood''~\cite{ATL-CONF-2011-034}.
The profile likelihood reduces the dimensionality of the parameter space by eliminating nuisance parameters while maintaining well-understood confidence intervals.

The profile likelihood ratio is defined as
% http://www.slac.stanford.edu/econf/C030908/papers/WEMT004.pdf
\begin{equation}
  \lambda(\sigma_{\ttbar}) = \frac{L(\sigma_{\ttbar}, \hat{\hat{\lum}}, \hat{\hat{\alpha}}_j)}{L(\hat \sigma_{\ttbar}, \hat \lum, \hat \alpha_j)}\,,
\end{equation}

The profile likelihood techniques transforms a full likelihood function (including nuisance parameters) into a function resembling a likelihood of only parameters of interest my minimizing over the the nuisance parameters for each value of the parameter of interest (and including an overall scale of the global minimum of the likelihood function).
This construction is useful because its frequentist properties can be understood generally in the asymptotic regime.
Specifically, the distribution of this ratio under the data approaches a $\chi^2$ distribution as the size of the data increases (in practice, this convergence happens extremely quickly, which underlies the usefulness of the profile likelihood ratio).
Thus, a confidence interval for a parameter of interest can be obtained using the known properties of the $\chi^2$ distribution.
Specifically, a 65\% confidence interval is defined by the region about the global minimum between the points where the negative log likelihood of the profile likelihood ratio equals $-\frac{1}{2}$ of its maximum value.

This can be interpreted as searching for values of the nuisance parameters that make the probability of the data given the full set of parameters as large as possible.
Near the global minimum of the likelihood, this will have the effect of expanding the confidence interval of a parameter of interest since the the worse-case-scenario of the nuisance parameters will be chosen for each value of the parameter of interest, causing the likelihood function to be more flat around the global minimum, and therefore causing confidence intervals to be wider.

%\subsection{Systematic Uncertainties}
