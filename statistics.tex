


\section{Statistics}

Properly using statistical tools ... blah blah blah

Experimental High Energy Physics is uniquely suited for precise, reliable, and accurate statistical analysis.
The detailed knowledge of the underlying physics allow one to create extremely accurate probabilistic models.
Large datasets allow these models to be checked, calibrated, and finely tuned.
Sufficient computational resources are available for massive amounts of simulation to be generated from different models or similar models with various sets of parameters.
Finally, there are a plethora of statistical formulae, tools, and techniques dedicated to the types of measurements performed by High Energy Experimental Physicists.
The above advantages make ATLAS a [blah] environment for statistical analysis.

ATLAS uses a variety of standard statistical techniques for performing measurements.
The standard set at ATLAS is to use ``frequentist'' techniques, though this is an extremely loaded term.
Perhaps it is better to say that ATLAS uses ``likelihood'' techniques (a term which is both more specific and leads to less controversy).
In practice, this means that measurements are made by constructing a likelihood function, which is a real valued function, $L(data | \vec{\alpha})$, whose value is ves the probability of a data point or a set of data points as a function of a number of parameters, $\vec{\alpha}$.

It should be noted that a likelihood and a probability distribution function (pdf) are not synonomous.
A pdf is required to be strictly normalized such that:

\begin{equation}
\int pdf(\vec{x}) d\vec{x} = 1
\end{equation}

This must be true of any given pdf.
In particular, given a family of pdf's that are paramaterized by a real value $\alpha$, each pdf must obey:

\begin{equation}
\int pdf(\vec{x}, \alpha) d\vec{x} = 1 \hspace{5mm} \forall \alpha
\end{equation}

In contrast, likelihoods aren't normalized in this way.
The reason for this difference becomes clear if one interprets a likelihood as a functional of a pdf which maps that pdf onto a dataset to produce a real (positive) number:

\begin{equation}
L: \text{ \{pdf, data\} } \rightarrow \mathbb{R}
\end{equation}

Likelihoods only become useful objects when they are manipulated to produce statistical statements.
The type of statement that can be made from likelihoods is the source of disagreement between the ``bayesian'' and ``frequentist'' statistical sects.
We will here take an agnostic position on the philosophical merits of any type of statistical statement, and instead present a common type of statistical
statement, used and recommended by the ATLAS collaboration, as well as many other experiments, known as the confidence interval.

A confidence inverval is a procedure that maps a likelihood to a set in the space of parameters which describe the pdf domain of the likelihood function.
This procedure is useful because it has the following property: given many randomly-generated realizations of the dataset described by the likelihood, the
set obtained using the confidence interval procedure will contain the true value of a parameter (the one which describes the true pdf) a fixed percentage
of the time, where that percentage is known in advance and determines the procedure for obtaining confidence intervals.

In other words, assuming that we knew the true distribution for a dataset that we're interested in, the following procedure is possible:
\begin{itemize}
  \item Assume there exists true pdf, $pdf_{true} = pdf(\vec{\alpha}_{true})$, which describes how a single realization of a dataset was generated
  \item Create a likelihood function over the space of pdf's and datasets, assuming that the true pdf is contained in the domain of this likelihood function
  \item Choose a confidence level, which is a value between 0 and 1.
  \item There exists a specific procedure, based on that chosen confidence level, which produces a set $S_i$ in the space of parameters $\vec{\alpha}$ using our likelihood function, which we denote as the ``confidence interval''.  
  \item Using the true pdf, generate many datasets, and for each dataset, create a set in parameter space $S_i$ based on the above procedure.
  \item Given this set of intervals, {$S_i$}, count the fraction which contain $\alpha_{true}$.
  \item As the number of generated datasets approaches infinity, this fraction will approach the confidence level.
\end{itemize}

Confidence intervals are only useful if one finds the above property useful.
Stricly speaking, a confidence interval alone can not be used to evaluate the value or utility of a decision based on observed data.
They are, however, objective means of quantifying data, in the sense that they don't depend on assumptions about parameters other than those that are build into the likelihood function.
%Any other way of arguing for their merit or usefulness is strictly philosophical.
%We will here assume that confidence intervals are a desirable quantity to calculate and will show how they can be obtained for likelihoods that describe data obtained using the ATLAS detector.

\subsection{Neyman Construction}

A standard way of conceptualizing and constructing confidence intervals is through the use of a ``Neyman Construction.'' 

%% http://inspirehep.net/record/582577
%% @article{Conrad:2002kn,
%%       author         = ``Conrad, Jan and Botner, O. and Hallgren, A. and Perez de los Heros, Carlos'',
%%       title          = ``{Including systematic uncertainties in confidence
%%                         interval construction for Poisson statistics}'',
%%       journal        = ``Phys.Rev.'',
%%       volume         = ``D67'',
%%       pages          = ``012002'',
%%       doi            = ``10.1103/PhysRevD.67.012002'',
%%       year           = ``2003'',
%%       eprint         = ``hep-ex/0202013'',
%%       archivePrefix  = ``arXiv'',
%%       primaryClass   = ``hep-ex'',
%%       SLACcitation   = ``%%CITATION = HEP-EX/0202013;%%'',
%% }

%% http://inspirehep.net/record/454197
%% @article{Feldman:1997qc,
%%       author         = ``Feldman, Gary J. and Cousins, Robert D.'',
%%       title          = ``{A Unified approach to the classical statistical analysis
%%                         of small signals}'',
%%       journal        = ``Phys.Rev.'',
%%       volume         = ``D57'',
%%       pages          = ``3873-3889'',
%%       doi            = ``10.1103/PhysRevD.57.3873'',
%%       year           = ``1998'',
%%       eprint         = ``physics/9711021'',
%%       archivePrefix  = ``arXiv'',
%%       primaryClass   = ``physics.data-an'',
%%       reportNumber   = ``HUTP-97-A096'',
%%       SLACcitation   = ``%%CITATION = PHYSICS/9711021;%%'',
%% }

%% http://inspirehep.net/record/1081445
%% @article{Rolke:2011na,
%%       author         = ``Rolke, Wolfgang A. and Lopez, Angel M.'',
%%       title          = ``{Estimating a Signal In the Presence of an Unknown Background}'',
%%       journal        = ``Nucl.Instrum.Meth.'',
%%       volume         = ``A685'',
%%       pages          = ``16-21'',
%%       doi            = ``10.1016/j.nima.2012.05.029'',
%%       year           = ``2012'',
%%       eprint         = ``1112.2299'',
%%       archivePrefix  = ``arXiv'',
%%       primaryClass   = ``physics.data-an'',
%%       SLACcitation   = ``%%CITATION = ARXIV:1112.2299;%%'',
%% }

A Neyman Construction is a set of points in the set of parameters and possible measured data, as well as a procedure for using those points to obtain confidence intervals.
The construction is build as follows:
Consider a likelihood function over a space of parameters, $\vec{\theta}$, and over a space of measured data, $\vec{d}$: $L = L(\vec{d} | \vec{\theta} )$.
Pick a value $\alpha$ which represents the size (strength) of the confidence interval to be built.  This must be agreed upon in advance.
Then, for each point in parameter space, $\vec{\theta}$ (which defines a fixed likelihood function), create an a region in the set of data, ${\vec{d}}$, known as an ``acceptance region'', which is constructed such that the total probability measure of that region is $\alpha$: ${\vec{d}_{\vec{\theta}} : P(\vec{d} \ele \vec{d}_{\vec{\theta} | \vec{\theta}) = \alpha$.
This requires knowing, or at least being able to integrate, the likelihood of data at a fixed point $\vec{\theta}$ (this distribution can be obtained using Monte-Carlo techniques once $\vec{\alpha}$ has been specified).
Of course, there are many (often infinitely many) such regions for a given parameter point.
Therefore, one must decide in advance how an acceptance region is to be constructed given a fixed likelihood function.
Often, this decision is made by creating an ordering rule, which determines the order in which points are used to populate a the region, which is then filled in order until it has total measure $\alpha$.

An acceptance region is constructed for all possible values of $\vec{\theta}$.
The union of every such region (including both the data and the parameter points) is known as the confidence belt: confidence belt = $\union {(\vec{d}, \vec{\theta}) : (\vec{d}, \vec{\theta}) \ele $ some acceptance region for some $\vec{\theta} }$.
%(the name comes from the cases where there is 1 parameter and 1 data value in the likelihood; in more complicated situations, it is better described as a volume or a multidimensional set).
%We do this for all values of theta to define a belt (or, if theta is a vector, we define a volume in parameter space).

Using this set, a confidence region for a given measured data point, $\vec{d}_0$, can be constructed as the union of all points in the confidence belt which include the measured data point: ${(\vec{theta}, \vec{d}) \ele belt : d = d_{0}}$.
Specifically, confidence region for a particular parameter, $\alpha$, is union of all values of alpha in the confidence belt that contain $\vec{d}_0$.

If $\theta_0$ were the true value, all the data would fall in the interval in defined by $\theta_0$ $\apha$ percent of the time.
In addition, if the data were to fall in the confidence region of $\theta_0$, then the interval of parameter points would include $\theta_0$.
These two imply the statement: $\theta_0$ is in the confidence interval $\iff$ the data falls in the region of the likelihood function L($\theta_0$).
Thus, the coverage of the region is identical to the coverage of the interval, which is $\alpha$.
Therefore, the confidence interval defined by the above construction is guaranteed proper coverage.

In this sense, a confidence interval is the inversion of a hypothesis.
It represents the set of all points in parameter space for which the measured data falls within an acceptance region of size $\alpha$ (given some ordering rule).
For any single parameter in parameter space, the confidence interval is the the union of all points of that parameter that fall in the confidence band (confidence volume) in the full parameter space.


\subsection{Hypothesis Testing}
% Kyle:  Said yet another way, to claim a discovery, the confidence interval for the nuisance parameter(s) must be empty (when the construction is made assuming the null hypothesis).

\subsection{Systematic Uncertainties}



\subsection{Profile Likelihood}


